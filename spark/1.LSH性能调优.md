# 用户相似性求解 #

背景：推荐算法中的协同过滤，希望加入根据相似用户推荐商品，以提高泛化能力。要求为数千万用户中的每个人，找到最相似的一批人，基于这批人的行为，为用户推荐商品。公司使用Spark技术栈，目前社区中针对Spark MLlib的介绍往往比较简略，在此分享技术路线。

环境：Spark 2.4.0



## 共现矩阵方案 ##

最初的实现方案是基于共现矩阵，求用户的杰卡德距离（Jaccard Distance），过滤得出最相似的用户

```scala
// 计算A∩B的共现次数
val bothDF = df1
	.join(df2, col("second_id1") === col("second_id2"))
	.filter("primary_id1 <> primary_id2")
	.groupBy("primary_id1", "primary_id2")
	.agg(size(collect_set("second_id2")).as(CNT_BOTH))
	.filter(col(CNT_BOTH) >= lit(MIN_OCCURRENCE))
// 计算A单独出现的次数
val distinctDF1 = df1.groupBy("primary_id1").agg(countDistinct("second_id1").as(CNT_ROW))
// 计算B单独出现的次数
val distinctDF2 = df1.groupBy("primary_id2").agg(countDistinct("second_id2").as(CNT_COLUMN))

// 计算jaccard距离
col(CNT_BOTH) / (col(CNT_ROW) + col(CNT_COLUMN) - col(CNT_BOTH))
```

但这种方法存在以下弊端

+ 基于共现矩阵，则特征必须是离散型 （categorical），对连续型（numerical）特征不友好
+ 只适合少量特征，随着特征增多，join代价逐渐升高

在解决的过程中，我做了如下尝试：

+ 使用Bucketizer分桶，将连续型特征分桶，转换为离散型特征，继续使用**共现矩阵**计算用户相似性
+ 使用One Hot独热码，将离散型特征进行独热编码，转换为连续型特征，得到**用户向量**，计算欧式距离（Euclidean Distance）

### Bucketizer分桶优化 ###

使用Spark ML中的Bucketizer分桶，可以解决连续型特征的问题，但当特征大幅度增加时，对资源要求太高，最终放弃

```scala
// TODO
```

## 用户向量方案 ##

### 构建用户向量 ###

使用Spark ML中的OneHotEncoderEstimator独热编码，将离散型特征转换为连续性特征，并和原连续型特征合并，共同组成用户向量

用户向量可以用于用户相似性计算，也可以用于CTR模型训练，具有较强的通用性

在数据处理的过程中，还用到了StringIndexer、 VectorAssembler、 StandardScaler，分别用于建立One Hot的索引，合并特征和数值归一化

***学习心得***

+ handleInvalid使用：
  + StringIndexer中handleInvalid = “keep”，遇到未知特征会归入单独一类
  + VectorAssembler中handleInvalid = “keep”，需要配合VectorSizeHint使用，否则会由于向量大小未知抛出Exception
  + StandardScaler中handleInvalid = “keep”，遇到异常会归入NaN
  + 非必要情况下，不建议使用handleInvalid = “skip”，这种情况下该数据会被丢弃
+ NaN问题：
  + 向量中包含NaN，在使用Vectors.sqdist计算向量距离时，会返回NaN
  + 向量中包含NaN，在使用LSH生成用户Hash Vector时，会返回NaN，进而引起严重的**数据倾斜**问题

```scala
// 采用Pileline方式处理机器学习流程
  val stagesArray = new ListBuffer[PipelineStage]()
  for (col <- categoricalColumns) {
    // 使用StringIndexer 建立类别索引
    val indexer = new StringIndexer()
      .setInputCol(col)
      .setOutputCol(s"${col}_index")
      .setHandleInvalid("keep")
    stagesArray.append(indexer)
  }
// 使用OneHotEncoder将分类变量转换为二进制稀疏向量
val encoder = new OneHotEncoderEstimator()
  .setInputCols(categoricalColumns.map(_ + "_index"))
  .setOutputCols(categoricalColumns.map(_ + "_vec"))
  .setDropLast(false)
stagesArray.append(encoder)
// 合并数值列
val assemblerInputs = categoricalColumns.map(_ + "_vec") ++ numericColumns
val numericAssembler: VectorAssembler = new VectorAssembler()
  .setInputCols(assemblerInputs)
  .setOutputCol("numeric_indexed")
  .setHandleInvalid("keep") // function since 2.4.0
stagesArray.append(numericAssembler)
// 数值列归一化处理
val scalar: StandardScaler = new StandardScaler()
  .setInputCol(numericAssembler.getOutputCol)
  .setOutputCol("numeric_scaled")
  .setWithStd(true)
  .setWithMean(true)
stagesArray.append(scalar)
val pipeline = new Pipeline()
pipeline.setStages(stagesArray.toArray)
val pipelineModel = pipeline.fit(basicDF)
pipelineModel.transform(basicDF)
```

归一化StandardScaler会很大程度影响算法效率，原因还在分析中，有以下猜测：

+ 使用数据归一化，会使得数据由SparseVector转换成DenseVector，且数据会变得非常不规整，在计算相似性时计算量明显增加
+ 使用数据归一化，会影响max(L2)，在LSH中会影响参数取值，需要针对性调参

### 用户相似性计算 ###

#### Cross Join ####

基于用户向量，可以计算用户相似性。最简单的方法是进行cross join，为每个用户计算与其他用户的距离，取最近的用户

cross join实现简单，但性能着实堪忧。

**学习心得**

+ Window.partitionBy
  + 在Key分布不均匀时会造成**数据倾斜**
  + join后的结果，先使用filter过滤分值较低的用户，再使用Window.partitionBy进行排序，这样可以显著减少**shuffle开销**

#### LSH局部敏感哈希 ####

为解决cross join的性能问题，我使用LSH，该算法将高维空间相似性问题转换为低维空间相似性问题，显著提升了计算效率

> References:
>
> (1) Gionis, Aristides, Piotr Indyk, and Rajeev Motwani. "Similarity search in high dimensions
>
> via hashing." VLDB 7 Sep. 1999: 518-529.
>
> (2) Wang, Jingdong et al. "Hashing for similarity search: A survey." arXiv preprint
>
> arXiv:1408.2927 (2014).

```scala
val brp = new BucketedRandomProjectionLSH()
    .setBucketLength(length)
    .setNumHashTables(numHash)
    .setInputCol("features")
    .setOutputCol("hashes")
val model = brp.fit(df1)
val tdf1 = model.transform(df1)
val tdf2 = model.transform(df2)
val hashResult = model.approxSimilarityJoin(tdf1, tdf2, threshold, "EuclideanDistance")
    .filter("datasetA.id <> datasetB.id")
  // 根据hash相似性结果，筛选最终结果
  val similarityResult = hashResult
    .select(
      col("datasetA.id"),
      col("datasetB.id"),
      col("EuclideanDistance")
     )
    .withColumn("rank", row_number().over(Window.partitionBy("datasetA.id").orderBy(col("EuclideanDistance"))))
    .filter(col("rank").leq(other = 15))
```

**学习心得**

+ BucketLength
  
  + **BucketLength决定了每个Hash桶的数枚举值个数**
  
  + BucketLength是Double类型，不仅可以是3.0、10.0 ，在必要的时候，也可以是0.001，特征值的选取与数据挂钩
  
    ```
    If input vectors are normalized, 1-10 times of pow(numRecords, -1/inputDim) would be a reasonable value
    ```
  
  + BucketLength越大，每个Hash桶里的枚举值越少，会有更多的用户能找到相似者，召回率会提高
  
    BucketLength越小，每个Hash桶里的枚举值越多，合理的BucketLength，可以使用户分布在更多的枚举值上，有效减少**join时的shuffle代价**
    + 假设数据量为1000万条，如果每个Hash桶得到10000个枚举值，此时每个桶大概会分配1 000条数据（实际情况可能并不会均匀分布），我在1000条里找到最接近的数据即可，此时寻找最近邻就由1000万 x 1000万，降维成10000 x 1000 x 1000的问题
    + 同理，如果每个Hash桶得到100个枚举值，此时每个桶大概会分配100000条数据，此时寻找最近邻就由1000万 x 1000万，降维成100 x 10万 x 10万的问题
    + 枚举值越多，准确率越低，性能越好，枚举值越低，准确率越高，性能越差，如果枚举值=1，则等同于cross join
  
  + 根据下面公式，如果数据为1000万条，2000维，则最大L2范数大致在1-40之间，为减少join时结果数量过大，希望桶数稍微多一些，如10000个桶，此时`BucketLength= max(L2) / bucketNum = 0.0001-0.004`
  
  + ```
    The length of each hash bucket, a larger bucket lowers the false negative rate. The number of buckets will be `(max L2 norm of input vectors) / bucketLength`.
    ```
  
+ numHash

  + numHash决定了Hash桶的个数，增加更多的Hash桶，会使用更多中Hash函数，减少单个Hash函数对结果的影响。numHash越多，准确率越高，性能越差。

  + ```
    number of hash tables, where increasing number of hash tables lowers the false negative rate, and decreasing it improves the running performance
    ```

+ transform
  + 建议在LSH.approxSimilarityJoin前，对输入数据进行transform，这样能检查transform结果是否正确
  + LSH.approxSimilarityJoin能处理未transform的dataframe，在LSH.processDataset功能中，会自动transform输入的dataframe

##  Spark调参心得

+ join过程会产生宽依赖，如果未发生数据倾斜，增加shuffle.partitions可以显著提升效率，我将shuffle.partitions由50->2000，节点分配到的数据由130万下降到5万，算法由1h->10min

  ```--conf spark.sql.shuffle.partitions=2000```

+ shuffle.partitions调整时需要考虑每个节点分配到的数据
  + 每个节点分配到的数据都很少
    + 说明整体数据很小，较长时间用于分配executor，此时应降低partitions
  + 某个节点分配到的数据很多，其他很少
    + 说明出现数据倾斜，需要找到较大的key来分析原因，常见的有
      + NULL、NaN导致空值倾斜，此时应提前处理空值
      + Big Key导致倾斜，此时需要对Big Key区分对待，如拆分仅含Big Key的Dataframe和无Big Key的Dataframe，分而治之，或者提前拆分Big Key

+ 数据量较小时可以采用Broadcast，使用map join避免shuffle，但用户相似性计算中不包括小表join大表的情况，所以未使用map join，下面命令可以关闭map join

  `--conf spark.sql.autoBroadcastJoinThreshold=-1`

+ 另外，被广播的表需要先collect到driver，再分发到executor，当表较大时，会对driver和executor同时造成压力，需要适当增加driver.maxResultSize来配合map join

  `--conf spark.driver.maxResultSize=10g`

+ 基础表不能被广播，比如left outer join时，只能广播右表

  `--conf spark.sql.autoBroadcastJoinThreshold`

+ 启动推测执行，避免因为服务器负载问题导致任务延迟，可以启动推测执行。但相似性计算的计算量很大，尤其是在发生数据倾斜时，过早启用推测执行会占用更多资源，所以quantile取95%

  ```
  --conf spark.speculation=true \
  --conf spark.speculation.interval=2s \
  --conf spark.speculation.multiplier=100 \
  --conf spark.speculation.quantile=0.95 \
  ```